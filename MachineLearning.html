<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<style>
.sidenav {
  height: 100%;/* Full-height: remove this if you want "auto" height */
  width: 160px; /* Set the width of the sidebar */
  position: fixed; /* Fixed Sidebar (stay in place on scroll) */
  z-index: 1; /* Stay on top */
  top: 10%; /* Stay at the top */
  left: 0;
  background-color: #111; /* Black */
  overflow-x: hidden; /* Disable horizontal scroll */
  padding-top: 20px;
}

/* The navigation menu links */
.sidenav a {
  padding: 6px 8px 6px 16px;
  text-decoration: none;
  font-size: 18px;
  color: #818181;
  display: block;
}

/* When you mouse over the navigation links, change their color */
.sidenav a:hover {
  color: #f1f1f1;
}

.main {
  margin-left: 160px ;
}

.alt style2 {
  margin-left: 160px ;
}



/* Style page content */


/* On smaller screens, where height is less than 450px, change the style of the sidebar (less padding and a smaller font size) */
@media screen and (max-height: 450px) {
  .sidenav {padding-top: 15px;}
  .sidenav a {font-size: 14px;}
}
	</style>
		<title>Nicholas Bandy</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<!-- Side navigation -->
<div class="sidenav">
  <a href="#discussion_1">Discussion 1</a>
  <a href="#seminar_2">Seminar 2</a>
  <a href="#jaccard">Jaccard Coefficient</a>
  <a href="#kmeans">K-Means</a>
  <a href="#seminar_3">Seminar 3</a>
</div>
	<body class="is-preload">
		<!-- Wrapper -->

				<!-- Header -->
				<!-- Note: The "styleN" class below should match that of the banner element. -->
					<header id="header" class="alt style2">
						<a href="index.html" class="logo"><strong>Nicholas Bandy</strong></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="dbd.html">Deciphering Big Data</a></li>
							<li><a href="MachineLearning.html">Machine Learning</a></li>
						</ul>
						
					</nav>

				<!-- Banner -->
				<!-- Note: The "styleN" class below should match that of the header element. -->
					<section id="banner" class="style2">
						<div class="inner">
							<span class="image">
								<img src="images/pic07.jpg" alt="" />
							</span>
							<header class="major">
								<h1>Machine Learning</h1>
							</header>
						</div>
					</section>

				<!-- Main -->
					<div id="main" class="main">

						<!-- One -->
							<section id="discussion_1">
								<div class="inner">
									<header class="major">
										<h2>Discussion 1</h2>
									</header>
									<p>Technology has become an integral part of our lives, and the rapid adoption of technology is expected to continue or even accelerate (Schwab & Zahidi, 2020). Our dependence on technology means that unforeseen errors or failures in technology can have major repercussions.</p>

<p>An example of this was seen with a signal failure in Kowloon which caused major delays for up to 4 hours while officials were unaware (Chi-fai, 2004). Discussing the incident with my peers, a common theme was the importance of proper oversight for technology and procedures in place for when technology fails to work as expected. Having humans involved in crucial processes to work alongside technology can maximize the performance by leveraging both the speed and efficiency of automation and the creativity and problem solving skills of a human during unique or unforeseen situations (Noble et at. 2022). This could have led to a swift resolution to the signal failure and allowed traffic to flow during rush hour, so that the technology failure could be addressed in a less busy time.</p>

<p>The security of iot devices was also discussed, as they are often susceptible to cyber threats (Baldini et. Al, 2017). There are a wide range of sensors used in transportation to control rail signals, count the number of passengers on board, or measure the crowding at a station. Designing these systems to minimize the amount of personal data being processed, for example using infrared sensors to count boardings instead of cameras to identify passengers and track their movements, can help to improve some of the privacy concerns around these technologies. Ensuring proper cybersecurity protocols are in place is important in protecting this data.</p>
										
  

<p>References</p>

<p>Baldini, G., Botterman, M., Neisse, R., & Tallacchini, M. (2016). Ethical design in the internet of things. Science and Engineering Ethics, 24(3), 905–925. Available From https://doi.org/10.1007/s11948-016-9754-5</p>

<p>Chi-fai, C. (2004). "Outrage at rush-hour rail chaos; West Rail signal failure delays 10,000, but authorities only hear about it from radio broadcasts". South China Morning Post. Available From advance.lexis.com/api/document?collection=news&id=urn:contentItem:4D44-BH60-0002-P131-00000-00&context=1519360</p>

<p>Noble, S., Mende, M., Grewal, D. & Parasuraman, A. (2022) The fifth Industrial Revolution: How harmonious Human-Machine Collaboration is Triggering a Retain and Service [R]evolution. Journal of Retailing 28(2): 199-208 Available From: https://doi.org/10.1016/j.jretai.2022.04.003</p>

<p>Schwab, K. & Zahidi, S. (2020) The Future of Jobs Report.  World Economic Forum. Available from: https://www3.weforum.org/docs/WEF_Future_of_Jobs_2020.pdf</p>									
								</div>
							</section>

						<!-- Two -->
						<section id="seminar_2">
								<div class="inner">
									<header class="major">
										<h2>Seminar 2</h2>
									</header>
									<p>In preparation for this seminar, our task was to perform exploratory data analysis (EDA) with the Auto-mpg dataset. Importing the required libraries and printing the first 5 rows of the datasets indicate that we expect the car name to be text, with the other columns being numeric.</p>
									<pre><code>import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import scipy.stats as st
from sklearn import ensemble, tree, linear_model
import missingno as msno
df.head()</code></pre>
									<img src="images/data head sem 2.PNG" alt="Diagram" style="width:100%">
  									<figcaption>Figure 1: first 5 rows of the Auto-mpg dataset</figcaption>
									<br>
									<p>Checking the datatypes for each column we can see that horsepower is saved as a text field. Printing distinct values showed that instead of using nan values, ‘?’ was used in cases the horsepower was unknown. ‘?’ was then replaced with null values so that horsepower could be converted to a numeric column. This leaves 6 rows where horsepower is null as the only null values in the dataframe. The skew for all columns is less than 3 and the kurtosis is less than 6, this indicates to us that the columns are all symmetrical and don’t have heavy tails (DeCarlo, 1997).</p>
									<pre><code>df.skew(), df.kurt()</code></pre>	
									<img src="images/skew and kurtosis sem2.PNG" alt="Diagram" style="width:30%">
  									<figcaption>Figure 2: skew and kurtosis for each column</figcaption>
									<br>
									<p>Generating a heatmap for this data shows that weight, horsepower, displacement, and cylinders have strong correlation with each other, while those four columns all also show a strong negative correlation with the origin, model year, and acceleration.</p>
<pre><code>f , ax = plt.subplots(figsize = (14,12))
plt.title('Correlation of Numeric Features',y=1,size=16)
sns.heatmap(df.corr())</code></pre>
									<img src="images/correlation heatmap sem2.png" alt="Diagram" style="width:100%">
  									<figcaption>Figure 3: heatmap for correlation between features</figcaption>
									<br>
									<p>A new column ‘car name code’ is added which converts the car name column to a numeric value.</p>
<pre><code>df['car_name_code'] = df['car name'].astype('category').cat.codes</code></pre>
									
									<p>Finally a pairplot is produced to visualize the relations between each variable. The correlations above are easily visible in the pairplot, while also showing the relationship between car name and origin.</p>
<pre><code>sns.set()
sns.pairplot(df,height = 2 ,kind ='scatter',diag_kind='kde')
plt.show()</code></pre>
									<img src="images/pairplot sem 2.png" alt="Diagram" style="width:100%">
  									<figcaption>Figure 4: pairplot to visualize relationships</figcaption>
									<br>
									<p>This exercise was helpful in providing an opportunity to apply the concepts we have learned in the early weeks of the course, and understanding the EDA process will be useful in understanding how to approach a new dataset in the future.</p>
									<a href="https://github.com/Nicholas-Bandy/eportfolio/blob/main/Seminar%202.ipynb">View the final code here</a> 

<p>References</p>

<p>DeCarlo, L. T. (1997) On the Meaning and Use of Kurtosis. Psychological methods. [Online] 2 (3), 292–307. Available From https://web-s-ebscohost-com.uniessexlib.idm.oclc.org/ehost/pdfviewer/pdfviewer?vid=0&sid=51b16d11-8f19-40e6-a38e-993423b297d6%40redis</p>
						
								</div>
							</section>

						<!-- Three -->
						<section id="jaccard">
								<div class="inner">
									<header class="major">
										<h2>Jaccard Coefficient</h2>
									</header>
									<p>This activity is to calculate the jaccard coefficient for each pair given the table below.</p>
									<pre>Name	Gender	Fever	Cough	Test-1	Test-2	Test-3	Test-4
Jack	M	Y	N	P	N	N	A
Mary	F	Y	N	P	A	P	N
Jim	M	Y	P	N	N	N	A
</pre>	
									
									<p>The Jaccard coefficient is equal to the sum of columns where either one person or the other tested positive but not both, divided by the sum of columns where one person or the other or both tested positive. Columns where both tested negative are not included.</p>
									
									<p>To set this table up for jaccard coefficient calculations, we replace positive values Y and P with 1, while negative values N and A are replaced with 0. Gender is omitted as it is not a positive or negative value.</p>
			<pre>Name	Fever	Cough	Test-1	Test-2	Test-3	Test-4
Jack	1	0	1	0	0	0
Mary	1	0	1	0	1	0
Jim	1	1	0	0	0	0
</pre>
									<p>Jack and Mary: Jack did not test positive for anything that Mary did not, and Mary only tested positive for fever where Jack tested negative. This gives a numerator of 1. They both tested positive for fever and test1, giving a denominator of 2 + 1 = 3. Therefore, the Jaccard coefficient is 1/3</p>
									<p>Jack and Jim: Jack tested positive for test1 where Jim tested negative, and Jim tested positive for cough where Jack tested negative. This gives a numerator of 2. They both tested positive for fever, giving a denominator of 2 + 1 = 3. Therefore, the jaccard coefficient is 2/3</p>
									<p>Mary and Jim: Mary tested positive for test1 and test3 where Jim tested negative, while Jim tested positive for cough where Mary tested negative. This gives a numerator of 3. They both tested positive for fever, giving a numerator of 3 + 1 = 4. Therefore, the jaccard coefficient is 3/4
		
								</div>
							</section>

						<!-- Three -->
						<section id="kmeans">
								<div class="inner">
									<header class="major">
										<h2>K-Means</h2>
									</header>
									<p> <a href="https://shabal.in/visuals/kmeans/1.html">The first kmeans visual</a> shows a distribution of data points and how the algorithm is affected by the starting point. Starting with the four points furthest left for the first run, furthest top for the second, furthest right for the fourth, and furthest bottom for the fourth. Only one of these gave the result of the 4 clusters that I had expected to see looking at the data points before the algorithm ran. This showed that picking the extremes as starting points can affect how the clusters are formed, as sometimes they would end up with small clusters preventing a centroid from moving closer to where it would give the most accurate match. The final run uses random starting points and does end up with the expected clusters, as the starting points are not at the extremes of the data set.</p>
				<p> <a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">For the second kmeans visual</a> we were to choose our own points for a uniform distribution of data. Regardless how many clusters I chose to have or where I picked to initialize them, I always finished with the same equal clusters. This highlighted the effect that the dataset has on an algorithm, and that for the uniform dataset the starting points were not important as it always came to the same result. </p>
									
								</div>
							</section>

						<!-- Two -->
						<section id="seminar_3">
								<div class="inner">
									<header class="major">
										<h2>Seminar 3</h2>
									</header>
									<p>In preparation for the third seminar, we were given three tasks that involved performing k-means clustering on a given dataset.</p>
									
									<p>Task A was to perform k-means clustering on the iris dataset (Fisher, 1988) using k=3, and evaluate the models performance. The dataset was imported directly from the UCI repo where it is stored, and split into targets and features as described in their import guide.</p>
									<pre><code>from ucimlrepo import fetch_ucirepo 
import random 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.cluster import KMeans 
from sklearn.preprocessing import StandardScaler
import pandas as pd

%matplotlib inline
  
# fetch dataset 
iris = fetch_ucirepo(id=53) 
  
# data (as pandas dataframes) 
X = iris.data.features 
y = iris.data.targets 
  
# metadata 
print(iris.metadata) 
  
# variable information 
print(iris.variables)</code></pre>	
									
									<p>There are no null values in the dataset, and since it is already split into features and a target there was no need to drop any columns as reccommended in the assignment. The dataset was then normalized in preparation for clustering.</p>
<pre><code>Clus_dataSet = StandardScaler().fit_transform(X)</code></pre>
									
									<p>k-means clustering can now be performed and datapoints assigned to a cluster.</p>
<pre><code>clusterNum = 3
k_means = KMeans(init = "k-means++", n_clusters = clusterNum, n_init = 12)
k_means.fit(X)
labels = k_means.labels_</code></pre>
									
									<p>To measure the performance of the clustering we compare the cluster labels to the corresponding class.</p>
<pre><code>Finaldf = y
Finaldf['cluster'] = labels

for item in pd.unique(y['class']):
  tempdf = Finaldf[Finaldf['class'] == item]
  print( item + 'Predicted 0 = ' + str(len(tempdf[tempdf['cluster'] == 0])))
  print( item + 'Predicted 1 = ' + str(len(tempdf[tempdf['cluster'] == 1])))
  print( item + 'Predicted 2 = ' + str(len(tempdf[tempdf['cluster'] == 2])))</code></pre>
									<img src="images/prediction seminar 3 taskA.PNG" alt="Diagram" style="width:50%">
  									<figcaption>Figure 1: model predictions for each class, iris dataset</figcaption>
									<br>
									<p>We can see that all Iris-setosa were predicted to be in class 0. Iris-versicolor had 48 predicted in class 1 and 2 predicted in class 2, giving a 96% accuracy. Iris-virginica had 36 predicted in class 2 and 14 predicted in class 1, giving a 72% accuracy. This clustering model was effective for classifying Iris-setosa, while it occasionaly miscategorized Iris-versicolor and often miscategorized Iris-virginica.</p>
									
									<p>Task B was to perform k-means clustering on the wine dataset (Aeberhard & Forina, 1991) using k=3, and evaluate the models performance. The dataset was imported directly from the UCI repo where it is stored, and split into targets and features as described in their import guide.</p>
									<p>The same import steps were followed as in task A, and the dataset was already split into numeric features and a target, so there were no columns to drop. There are also no null values in the dataset. After the data is normalized, clustering is performed with k=3</p>				
									<pre><code>clusterNum = 3
k_means = KMeans(init = "k-means++", n_clusters = clusterNum, n_init = 12)
k_means.fit(X)
labels = k_means.labels_</code></pre>
									<p>To evaluate the models performance, we look at the number of times each target class was predicted for each cluster</p>
									<pre><code>Finaldf = y
Finaldf['cluster'] = labels

for item in pd.unique(y['class']):
  tempdf = Finaldf[Finaldf['class'] == item]
  print('Class' + str(item) + ' Predicted 0 = ' + str(len(tempdf[tempdf['cluster'] == 0])))
  print('Class' + str(item) + ' Predicted 1 = ' + str(len(tempdf[tempdf['cluster'] == 1])))
  print('Class' + str(item) + ' Predicted 2 = ' + str(len(tempdf[tempdf['cluster'] == 2])))</code></pre>
									<img src="images/prediction seminar 3 taskB.PNG" alt="Diagram" style="width:50%">
  									<figcaption>Figure 2: model predictions for each class, wine dataset</figcaption>
									<p>Target class 1 was predicted for cluster 0 46 times and cluster 2 13 times, giving an accuracy of 78%. Target class 2 was predicted for cluster 0 once, cluster 1 50 times, and cluster 2 20 times, giving an accuracy of 70%. Target class 3 was predicted for cluster 1 19 times, and cluster 2 29 times, giving an accuracy of 60%</p>
									<p>Using clustering on the wine dataset gave much lower performance compared to the iris dataset, likely indicating that the different clusters are not as clearly defined by their characteristics when compared to the iris dataset.</p>
									
									<a href="https://github.com/Nicholas-Bandy/eportfolio/blob/main/Seminar3.ipynb">View the final code here</a> 

<p>References</p>

<p>Fisher,R. A.. (1988). Iris. UCI Machine Learning Repository. https://doi.org/10.24432/C56C76.</p>
									<p>Aeberhard,Stefan and Forina,M.. (1991). Wine. UCI Machine Learning Repository. https://doi.org/10.24432/C5PC7J.</p>
						
								</div>
							</section>

						
				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Name</label>
											<input type="text" name="name" id="name" />
										</div>
										<div class="field half">
											<label for="email">Email</label>
											<input type="text" name="email" id="email" />
										</div>
										<div class="field">
											<label for="message">Message</label>
											<textarea name="message" id="message" rows="6"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="primary" /></li>
										<li><input type="reset" value="Clear" /></li>
									</ul>
								</form>
							</section>
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<h3>Email</h3>
										<a href="#">nicholasbandy1@gmail.com</a>
									</div>
								</section>
								
							</section>
						</div>
					</section>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="icons">
								<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="#" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
